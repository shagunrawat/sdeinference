\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array, float}
\newcommand{\bx}{\ensuremath{\mathbf{x}}}
\newcommand{\bz}{\ensuremath{\mathbf{z}}}
\newcommand{\btheta}{\ensuremath{\boldsymbol{\theta}}}
\newcommand{\bLambda}{\ensuremath{\boldsymbol{\Lambda}}}
\newcommand{\bGamma}{\ensuremath{\boldsymbol{\Gamma}}}
\begin{document}
\vspace*{-15mm}
\title{em_calc}
\begin{center}
\Large\textbf{Expectation Maximization for the DTQ method} \\
\normalsize
\end{center}
We consider a parameteric SDE model:
\begin{equation}
dX_t = f(X_t; \btheta) dt + g(X_t; \btheta) dW_t
\end{equation}
In this model, $f(X_t; \btheta)$ is the drift function and $g(X_t; \btheta)$ is the diffusion function.  A concrete example of such an SDE is the Ornstein-Uhlenbeck SDE,
\begin{equation}
dX_t = \theta_1 (\theta_2 - X_t) dt + \theta_3 dW_t.
\end{equation}
We start with the parameter inference problem where we have data available as a time series, denoted by $\bx = (x_0, x_1, \ldots, x_N)$. Suppose the observed data has large inter-observation times.  Then we consider observations at intermediate times as \textit{missing data points}, denoted by $\bz$. On the interval $[t_i, t_{i+1}]$, we have two observed data points, $X_{t_i} = x_i$ and $X_{t_{i+1}} = x_{i+1}$. We consider $F$ missing data points on this interval, denoted by $z_{i,F}$, the first subscript corresponding to the interval and the second subscript for the missing data point on the interval. Thus the missing data on an interval $[t_i, t_{i+1}]$, can be represented as $\bz_i = (z_{i1}, z_{i2}, \ldots, z_{iF})$. The complete data on this interval would thus become $(x_i, z_{i1}, z_{i2}, \ldots, z_{iF}, x_{i+1})$, comprising the observed data and the unknown missing data that we introduced.

\section{EM algorithm}
The Expectation-Maximization algorithm consists of two steps, computing the expectation of the log likelihood function and maximizing this value with respect to the parameters.
\begin{enumerate}
\item Start with an initial guess for the parameter, $\btheta^{(0)}$
\item For the expectation step,
\begin{align}
\label{eqn:expectation}
Q(\btheta, \btheta^{(k)}) & = \mathbb{E}_{\bz \mid \bx, \btheta^{(k)}} [\log p(\bx, \bz \mid \btheta)] \\
& = \sum_{\bz} \underbrace{\log p(\bx, \bz \mid \btheta)}_{\text{Part I}} \cdot \underbrace{p(\bz \mid \bx, \btheta^{(k)})}_{\text{Part II}}
\end{align}
In the above expression and in what follows, we use $\sum_{\bz}$ to stand for either summation or integration over the random vector $\bz$, depending on whether $\bz$ is discrete or continuous, respectively.
\item For the maximization step, we start with the current iterate and a dummy variable $\btheta$, so that the next iterate of the parameters would be the maximal value of $\btheta$
\begin{align}
\label{eqn:maximization}
\btheta^{(k+1)} = \arg \max_{\btheta} Q(\btheta, \btheta^{(k)}) 
\end{align}
We can either use a numerical optimizer for the optimization step or differentiate the $Q(\btheta, \btheta^{(k)})$ function with respect to $\btheta$ vector and equate it to zero to get the maximal value.
\item Iterate Step 2 and 3 until convergence.
\end{enumerate}

\subsection{Why does this work?}
\label{sect:why}
We begin with
$$
p(\bx, \bz \mid \btheta) = p(\bz \mid \bx, \btheta) p(\bx \mid \btheta).
$$
This is an identity, valid by the laws of probability.  If we assume 
$p(\bz \mid \bx, \btheta) \neq 0$, then we can divide both sides by the
same and then take the log of both sides to arrive at
$$
\log p(\bx \mid \btheta) = \log p(\bx, \bz \mid \btheta) - \log p(\bz \mid \bx, \btheta).
$$
We now multiple both sides by the density $p(\bz \mid \bx , \btheta^{(k)})$ and sum over $\bz$---this is the same as computing, on both sides, the conditional expectation of $\bz$ given $\bx$ and $\btheta^{(k)}$.  The result is
$$
\log p(\bx \mid \btheta) = \sum_{\bz} \log p(\bx, \bz \mid \btheta) p(\bz \mid \bx , \btheta^{(k)}) - \sum_{\bz} \log p(\bz \mid \bx, \btheta) p(\bz \mid \bx , \btheta^{(k)}).
$$
Let us define
$$
H(\btheta \mid \btheta^{(k)}) = - \sum_{\bz} \log p(\bz \mid \bx, \btheta) p(\bz \mid \bx , \btheta^{(k)}).
$$
Then we have
$$
\log p(\bx \mid \btheta) = Q(\btheta \mid \btheta^{(k)}) + H(\btheta \mid \btheta^{(k)}).
$$
If we insert $\btheta = \btheta^{(k)}$ into this last equation, we obtain
$$
\log p(\bx \mid \btheta^{(k)}) = Q(\btheta^{(k)} \mid \btheta^{(k)}) + H(\btheta^{(k)} \mid \btheta^{(k)}).
$$
Now we subtract the last equation from the previous one to obtain
$$
\log p(\bx \mid \btheta) - \log p(\bx \mid \btheta^{(k)}) = Q(\btheta \mid \btheta^{(k)}) - Q(\btheta^{(k)} \mid \btheta^{(k)}) + H(\btheta \mid \btheta^{(k)}) - H(\btheta^{(k)} \mid \btheta^{(k)}).
$$
Let $a = p(\bz \mid \bx, \btheta^{(k)})$ and $b = p(\bz \mid \bx, \btheta)$.  Then we have
\begin{align*}
H(\btheta \mid \btheta^{(k)}) - H(\btheta^{(k)} \mid \btheta^{(k)}) &= \sum_{\bz} a \log a - a \log b \\
 &= - \sum_{\bz} a \log \frac{b}{a} = \mathbb{E}_a[\phi(b/a)] \\
 &\geq -\log \sum_{\bz} a \cdot \frac{b}{a} = \phi(\mathbb{E}_a[b/a]) \\
 &\geq -\log \sum_{\bz} b = -\log 1 = 0.
\end{align*}
The inequality above is Jensen's inequality applied to the convex function $\phi(x) = -\log x$.  The overall inequality for generic densities $a$ and $b$ is often called Gibbs' inequality.

With this fact, we have
$$
\log p(\bx \mid \btheta) - \log p(\bx \mid \btheta^{(k)}) \geq Q(\btheta \mid \btheta^{(k)}) - Q(\btheta^{(k)} \mid \btheta^{(k)}) 
$$
Now as long as we choose $\btheta$ such that
$$
Q(\btheta \mid \btheta^{(k)}) \geq Q(\btheta^{(k)} \mid \btheta^{(k)}),
$$
then we are guaranteed that
$$
\log p(\bx \mid \btheta) \geq \log p(\bx \mid \btheta^{(k)}).
$$
In words, what this says is that if $\btheta$ improves the value of the $Q$ function, then the same $\btheta$ improves the value of the (incomplete) log likelihood function as well.

\subsection{Computation of the complete log likelihood}
\label{sect:loglik}
The first part of the expectation is the complete likelihood, $\log p(\bx, \bz \mid \btheta)$, which can be expanded as,
\begin{align}
\label{eqn:loglik}
\log p(\bx, \bz \mid \btheta) = \log p(x_0 \mid \btheta) & + \underbrace{\sum_{i=0}^{N-1} \log p(z_{i1} \mid x_i, \btheta)}_{(1)} + \underbrace{\sum_{i=0}^{N-1} \sum_{j=1}^{F-1} \log p(z_{i,j+1} \mid z_{ij}, \btheta)}_{(2)} \nonumber \\ 
& + \underbrace{\sum_{i=0}^{N-1} \log p(x_{i+1} \mid z_{iF}, \btheta)}_{(3)} 
\end{align}
The expression can be simplified under the assumption that $F$ is sufficiently large so that we can make an assumption that one-step transition densities in $(1), (2)$ and $(3)$ follow Gaussian distribution. Thus all the terms, $p(z_{i1} \mid x_i, \btheta), p(z_{i,j+1} \mid z_{ij}, \btheta)$ and $p(x_{i+1} \mid z_{iF}, \btheta)$ can be expressed with a Gaussian function
\begin{equation}
\label{eqn:gaussian}
G^h_{\btheta}(x,y) = p(x \mid y, \btheta) = \frac{1}{\sqrt{2 \pi g^2(y, \btheta)h}} \exp \left( - \frac{1}{2 g^2(y, \btheta)h} (x - y - f(y, \btheta)h)^2 \right)
\end{equation}
In what follows, we will return to the question of how to evaluate this function on a spatially discrete grid.

\subsection{Computation of the density of the missing data points}
\label{sect:densz}
Looking back at the expectation equation (\ref{eqn:expectation}), the expected value if computed by summing over all $\bz$ values which is a nested integral. Since the log likelihood can be expanded in 4 terms, so the density $p(\bz \mid \bx, \btheta^{(k)})$ gets multiplied by each of these terms. Upon summing over all the values of $\bz$, there will be 3 steps of terms remaining, corresponding to the respective terms in the equation (\ref{eqn:loglik}), as described below,
\begin{enumerate}
\item Corresponding to term $(1)$, we have the term $p(z_{i1} \mid \bx, \btheta^{(k)})$. Using Bayes' theorem, we get
$$
p(z_{i1}, \bx \mid \btheta^{(k)}) = p(z_{i1} \mid \bx, \btheta^{(k)}) \cdot p(\bx \mid \btheta^{(k)}).
$$
This implies
\begin{equation}
\label{eqn:term1}
 p(z_{i1} \mid \bx, \btheta^{(k)}) = \frac{p(z_{i1}, \bx \mid \btheta^{(k)})}{p(\bx \mid \btheta^{(k)})}  = \frac{p(z_{i1}, \bx \mid \btheta^{(k)})}{p(x_0 \mid \btheta^{(k)}) \prod_{j=0}^{N-1} p(x_{j+1} \mid x_{j}, \btheta^{(k)})} 
\end{equation}
The numerator can be expanded using the Markov property:
\begin{align*}
& p(z_{i1}, \bx \mid \btheta^{(k)}) = p(z_{i1}, x_0, x_1, \cdots, x_N \mid \btheta^{(k)}) \\
& = p(x_0 \mid \btheta^{(k)}) \prod_{j = i+1}^{N-1} p(x_{j+1} \mid x_{j}, \btheta^{(k)}) \, p(x_{i+1} \mid z_{i1}, \btheta^{(k)}) \, p(z_{i1} \mid x_i, \btheta^{(k)}) \prod_{j=0}^{i-1} p(x_{j+1} \mid x_{j}, \btheta^{(k)})
\end{align*}
Substituting this expansion into (\ref{eqn:term1}), we obtain
\begin{equation}
\label{eqn:zi1term}
p(z_{i1}\mid \bx, \btheta^{(k)}) = \frac{p(x_{i+1} \mid z_{i1}, \btheta^{(k)}) \, p(z_{i1} \mid x_i, \btheta^{(k)})}{p(x_{i+1} \mid x_{i}, \btheta^{(k)})}
\end{equation}
\item Corresponding to the $F$ internal steps represented by term (2), we have the terms $p(z_{i,j+1}, z_{ij} \mid \bx, \btheta^{(k)})$. We again use Bayes' theorem to get
\begin{equation}
\label{eqn:term2}
p(z_{i,j+1}, z_{ij} \mid \bx, \btheta^{(k)}) = \frac{p(z_{i,j+1}, z_{ij}, \bx \mid \btheta^{(k)})}{p(x_0 \mid \btheta^{(k)}) \prod_{j=0}^{N-1} p(x_{j+1} \mid x_{j}, \btheta^{(k)})}
\end{equation}
The numerator can be expanded using the Markov property as follows
\begin{multline*}
p(z_{i,j+1}, z_{ij}, \bx \mid \btheta^{(k)}) = p(x_0 \mid \btheta^{(k)}) \prod_{j=0}^{i-1} p(x_{j+1} \mid x_j, \btheta^{(k)}) \cdot p(z_{ij} \mid x_i, \btheta^{(k)}) \cdot p(z_{i,j+1} \mid z_{ij}, \btheta^{(k)}) \\
p(x_{i+1} \mid z_{i,j+1}, \btheta^{(k)}) \prod_{j=1}^{N-1} p(x_{j+1} \mid x_{j}, \btheta^{(k)}) 
\end{multline*}
We insert this into the numerator of (\ref{eqn:term2}) to obtain
\begin{equation}
\label{eqn:z2pt}
p(z_{i,j+1}, z_{ij} \mid \bx, \btheta^{(k)}) = \frac{ p (x_{i+1} \mid z_{i,j+1}, \btheta^{(k)}) \, p (z_{i,j+1} \mid z_{ij}, \btheta^{(k)}) \, p (z_{ij} \mid x_i, \btheta^{(k)}) }{p (x_{i+1} \mid x_{i}, \btheta^{(k)})}
\end{equation}
\item The last term (3) corresponds to $p(z_{iF} \mid \bx, \btheta^{(k)})$.  Using the same techniques described above, we can write this term as
\begin{equation}
\label{eqn:ziFterm}
p(z_{iF} \mid \bx, \btheta^{(k)}) = \frac{p(x_{i+1} \mid z_{iF}, \btheta^{(k)}) \, p(z_{iF} \mid x_{i}, \btheta^{(k)})}{p(x_{i+1} \mid x_{i}, \btheta^{(k)})}
\end{equation}
\end{enumerate}

\subsection{DTQ Computations}
Now for the part where we explain how the DTQ method enables us to calculate the terms above.  We focus on two terms in particular.  In what follows, we will need several definitions.  Let $\Delta x > 0$ denote the spatial grid spacing. Then the spatial grid is defined by $y_j = j \Delta x$ for $j = -M, \ldots, M$.  In this scheme, there are $2M+1$ grid points.  Recall the definition of $G^h_{\btheta}$ from (\ref{eqn:gaussian}).  This gives rise to the $(2M+1) \times (2M+1)$ matrix $K$ where
$$
K_{ab} = (\Delta x) G^h_{\btheta}(a \Delta x, b \Delta x)
$$
for $a, b \in \{-M, \ldots, M\}$.  Note the prefactor of $\Delta x$.  We also define the $(2M+1) \times 1$ vector $\bLambda^i$ by 
$$
\bLambda^i_a = G^h_{\btheta}(a \Delta x, x_i).
$$
Finally, we define the $1 \times (2M+1)$ vector $\bGamma^i$ by
$$
\bGamma^i_a = (\Delta x) G^h_{\btheta}( x_{i+1}, a \Delta x ).
$$
With these definitions, we have the DTQ approximation to the likelihood:
$$
p(x_{i+1} \mid x_i, \btheta^{(k)}) \approx \bGamma^i K^{F-1} \bLambda^i.
$$

\paragraph{Type I (forward).} The term we have in mind here is
$$
p(z_{ij} \mid x_i, \btheta^{(k)}).
$$
To get from time $t_i$ to the time associated with $z_{ij}$ requires $j$ steps of size $h$.  The first such step is handled analytically and the result is $\bLambda^i$.  Hence only $j-1$ steps remain to reach the time associated with $z_{ij}$.  Therefore,
$$
p(z_{ij} \mid x_i, \btheta^{(k)}) \approx K^{j-1} \bLambda^i.
$$
To compute the right-hand side, we start with the column vector $\bLambda^i$ and repeatedly multiply on the left by the matrix $K$.

\paragraph{Type II (backward).} The term we have in mind here is
$$
p(x_{i+1} \mid z_{i,j+1}, \btheta^{(k)}).
$$
Here we think of initializing $(2M+1)$ parallel trajectories at the time 
associated with $z_{i,j+1}$.  The $a$-th such trajectory is initialized with a Dirac delta centered at $a \Delta x$.  Then, at the time associated with $z_{i,j+2}$, we have a matrix of Gaussians.  Let us denote this matrix by
$$
A = \begin{bmatrix} G^h_{\btheta}( \mathbf{z}, -M \Delta x) \cdots
G^h_{\btheta}( \mathbf{z}, M \Delta x ) \end{bmatrix}.
$$
To be perfectly clear, the $j$-th column of $A$ is the $(2M+1) \times 1$ column vector
$$
G^h_{\btheta}( \mathbf{z}, j \Delta x)
$$
obtained by evaluating $G^h_{\btheta}( a \Delta x, j \Delta x)$ for each value of $a \in \{-M, \ldots, M\}$.  At this point, we  recognize that$$
A_{ab} = G^h_{\btheta} ( a \Delta x, b \Delta x),
$$
implying that
$$
A = (\Delta x)^{-1} K.
$$
Now that we have the $A$ matrix, we can write
$$
p(x_{i+1} \mid z_{i,j+1}, \btheta^{(k)}) \approx \bGamma^i K^{F - (j+2)} A 
= (\Delta x)^{-1} \bGamma^i  K^{F - (j + 1)}.
$$
To compute the quantity on the right-hand side, we start with the row vector $\bGamma^i$.  We repeatedly multiply on the right by the matrix $K$.  Finally, we multiply on the right by the matrix $A$.

Note that we need $1$ step to get from $z_{i,j+1}$ to $z_{i,j+2}$, $F-(j+2)$ steps to get from $z_{i,j+2}$ to $z_{iF}$, and then one last $\bGamma^i$ step to get to time $t_{i+1}$.

\paragraph{Important Point.} In the expressions we have given for $\bGamma^i$, $\bLambda^i$, $K$, and $A$, we have not specified the actual value to use for the parameter vector $\btheta$.  This is to be determined by context.  If we are conditioning on $\btheta^{(k)}$, then we should use $\btheta = \btheta^{(k)}$ consistently in $\bGamma^i$, $\bLambda^i$, $K$, and $A$.

\paragraph{Self-Consistency.} Let us examine the self-consistency of our derivations.  Using the laws of probability and the Markov property, we can write
$$
p(x_{i+1} \mid x_i, \btheta^{(k)}) = \int_{z_{ij}} p(x_{i+1} \mid z_{ij}, \btheta^{(k)}) p( z_{ij} \mid x_i, \btheta^{(k)} ) \, dz_{ij}.
$$
Suppose we now insert all of the approximations we have derived.  The integral becomes a summation and we replace $dz_{ij}$ with $\Delta x$.  On the right-hand side, we obtain the approximation
$$
(\Delta x)^{-1} \bGamma^i K^{F - j} \cdot K^{j-1} \bLambda^i \cdot \Delta x = 
\bGamma^i K^{F-1} \bLambda^i,
$$
which is exactly identical to our approximation of the left-hand side.  Hence our method is self-consistent.  One consequence of this is that densities such as
$$
p(z_{ij} \mid \mathbf{x}, \btheta^{(k)})
$$
computed using our method will be correctly normalized.

\paragraph{How to Actually Compute What We Want.}
The first term we actually want is (\ref{eqn:zi1term}).  
Given two vectors $\mathbf{a}$ and $\mathbf{b}$ of the same length---whether they are row or column vectors---we define their component-wise (or Hadamard) product to be the vector $\mathbf{a} \circ \mathbf{b}$ given by
$$
(\mathbf{a} \circ \mathbf{b})_i = a_i b_i.
$$
The vector $(\mathbf{a} \circ \mathbf{b})$ can be either a column or row vector, as appropriate.  With this notation, we have
\begin{align*}
p(z_{i1}\mid \bx, \btheta^{(k)}) &= \frac{p(x_{i+1} \mid z_{i1}, \btheta^{(k)}) \, p(z_{i1} \mid x_i, \btheta^{(k)})}{p(x_{i+1} \mid x_{i}, \btheta^{(k)})} \\
 &= \frac{ (\Delta x)^{-1} \bGamma^i K^{F-1} \circ \bLambda^i } { \bGamma^i K^{F-1} \bLambda^i }.
\end{align*}
This is term (\ref{eqn:zi1term}) above.  We can handle (\ref{eqn:ziFterm}) similarly:
\begin{align*}
p(z_{iF} \mid \bx, \btheta^{(k)}) &= \frac{p(x_{i+1} \mid z_{iF}, \btheta^{(k)}) \, p(z_{iF} \mid x_{i}, \btheta^{(k)})}{p(x_{i+1} \mid x_{i}, \btheta^{(k)})} \\
 &= \frac{ (\Delta x)^{-1} \bGamma^i \circ K^{F-1} \bLambda^i }{ \bGamma^i K^{F-1} \bLambda^i }.
\end{align*}
There is a slight increase in complexity when we seek to compute (\ref{eqn:z2pt}).  Let us start by recognizing that the quantity
$$
p(z_{i,j+1}, z_{ij} \mid \bx, \btheta^{(k)})
$$
will be represented by a $(2M+1) \times (2M+1)$ matrix.  Based on the right-hand side of (\ref{eqn:z2pt}), we start with
$$
p(z_{i,j+1} \mid z_{ij}, \btheta^{(k)}) = G^h_{\btheta^{(k)}}(z_{i,j+1}, z_{ij}).
$$
Discretizing the variables $z_{i,j+1}$ and $z_{ij}$ using our equispaced grid results in the same matrix $A = (\Delta x)^{-1} K$ discussed above.  Now let us add in the remaining pieces of (\ref{eqn:z2pt}).  Consider
\begin{align*}
z_{i,j+1} &= a (\Delta x) \\
z_{ij} &= b (\Delta x)
\end{align*}
for $a, b \in \{-M, \ldots, M\}$.  Then our approximation of $p(z_{i,j+1}, z_{ij} \mid \bx, \btheta^{(k)})$ is the matrix $\Psi$ given by
\begin{equation}
\label{eqn:Psidef}
\Psi_{a,b} = (\Delta x)^{-1} \frac{ \left[ \bGamma^i K^{F-(j+1)} \right]_a A_{ab} \left[ K^{j-1} \bLambda^i \right]_b }{ \bGamma^i K^{F-1} \bLambda^i }.
\end{equation}
For vectors $\mathbf{a}$ and $\mathbf{b}$, let $\mathbf{a} \otimes \mathbf{b}$ denote the Kronecker product.  This product is defined by $[\mathbf{a} \otimes \mathbf{b}]_{ij} = a_i b_j$.  Then we can write:
$$
p(z_{i,j+1}, z_{ij} \mid \bx, \btheta^{(k)}) \approx \Psi = \frac{ (\Delta x)^{-1} } { \bGamma^i K^{F-1} \bLambda^i } \left( \left[ \bGamma^i K^{F-(j+1)} \right] \otimes \left[ K^{j-1} \bLambda^i \right] \right) \circ A.
$$
Here the Hadamard product $\circ$ is an element-wise product between two matrices of equal size.

\subsection{Expectation Step}
Combining the terms from Section \ref{sect:loglik} and Section \ref{sect:densz}, we can form a complete expression for the expectation. Going back to Section \ref{sect:loglik}, we recall that the transition densities can be assumed to be Gaussian for sufficiently large enough $F$. Thus, the expectation expression can be rewritten as,
\begin{align}
Q(\btheta, \btheta^{(k)}) = \sum_{\bz} \Big \{ \log p(x_0 \mid \btheta) & + \sum_{i=0}^{N-1} \log G(z_{i1}, x_i, \btheta) \cdot p(z_{i1} \mid \bx, \btheta^{(k)}) \nonumber \\
& + \sum_{i=0}^{N-1} \sum_{j=1}^{F-1} \log G(z_{i,j+1}, z_{ij}, \btheta) \cdot p(z_{i,j+1}, z_{ij} \mid \bx, \btheta^{(k)}) \nonumber \\
& + \sum_{i=0}^{N-1} \log G(x_{i+1}, z_{iF}, \btheta) \cdot p(z_{iF} \mid \bx, \btheta^{(k)}) \Big \}
\end{align}

\subsection{Maximization Step}
For the maximization step, there are 2 ways to maximize $Q(\btheta, \btheta^{(k)})$, either through numerical optimizers or through equating the derivative with respect to the parameters to zero and using a root-finding solver if required.

Since both these methods require the gradient of $Q(\btheta, \btheta^{(k)})$ so we specify the gradients below. The derivative of $Q(\btheta, \btheta^{(k)})$ with respect to the $\btheta$ parameters would then be,
\begin{align*}
0 = \frac{\partial Q(\btheta, \btheta^{(k)})}{\partial \theta_{\ell}} = \frac{p'(x_0 \mid \btheta)}{p(x_0 \mid \btheta)} & + \sum_{i=0}^{N-1} \frac{H_{\ell}(z_{i1}, x_i, \btheta)}{G(z_{i1}, x_i, \btheta)} \cdot p(z_{i1} \mid x_i, \btheta^{(k)}) \\
& + \sum_{i=0}^{N-1} \sum_{j=1}^{F-1} \frac{H_{\ell}(z_{i,j+1}, z_{ij}, \btheta)}{G(z_{i,j+1}, z_{ij}, \btheta)} \cdot p(z_{i,j+1}, z_{ij} \mid \bx, \btheta^{(k)}) \nonumber \\
& + \sum_{i=0}^{N-1} \frac{H_{\ell}(x_{i+1}, z_{iF}, \btheta)}{G(x_{i+1}, z_{iF}, \btheta)} \cdot p(z_{iF} \mid \bx, \btheta^{(k)})
\end{align*}
where, $$H_{\ell}(x,y,\btheta) = \frac{\partial G(x,y,\btheta)}{\partial \theta_{\ell}} = \frac{\partial G}{\partial f}\cdot \frac{\partial f}{\partial \theta_{\ell}} + \frac{\partial G}{\partial g} \cdot \frac{\partial g}{\partial \theta_{\ell}} $$
\begin{enumerate}
\item With respect to $\theta_1$, $\theta_2$
\begin{align*}
\frac{H_{1}}{G}(x,y,\btheta) = \frac{\partial f}{\partial \theta_1} \left[ \frac{(x - y - f(y)h)}{g^2(y)} \right], \frac{H_{2}}{G}(x,y,\btheta) = \frac{\partial f}{\partial \theta_2} \left[ \frac{(x - y - f(y)h)}{g^2(y)} \right]
\end{align*}
\item With respect to $\theta_3$
\begin{align*}
\frac{H_{3}}{G}(x,y,\btheta) = \frac{\partial g}{\partial \theta_3} \left[ \frac{(x - y - f(y)h)^2}{h g^3(y)} - \frac{1}{g(y)} \right]
\end{align*}
\end{enumerate}

\subsection{Summary of terms that need to be computed}
\begin{enumerate}
\item $p(x_{i+1} \mid z_{i1}, \btheta^{(k)}) \cdot p(z_{i1} \mid x_{i}, \btheta^{(k)})$
\item $p(z_{ij} \mid x_i, \btheta^{(k)}) \cdot p(z_{i, j+1} \mid z_{ij}, \btheta^{(k)}) \cdot p(x_{i+1} \mid z_{i,j+1}, \btheta^{(k)})$
\item $p(x_{i+1} \mid z_{iF}, \btheta^{(k)}) \cdot p(z_{iF} \mid x_{i}, \btheta^{(k)})$
\item $p(x_{i+1} \mid x_{i}, \btheta^{(k)})$ - the complete DTQ computation
\item $\partial G/\partial f, \partial G/\partial g$ - computed in Dtheta function
\item $\partial f/\partial \theta_1, \partial f/\partial \theta_2, \partial g/\partial \theta_3$ - computed in Dtheta function
\end{enumerate}
















\end{document}
